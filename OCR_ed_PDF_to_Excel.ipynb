{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpuqN11aUJAQFheoYDd1jP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smypmsa/pdf-to-table/blob/main/OCR_ed_PDF_to_Excel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä PDF Table Extractor for Excel\n",
        "\n",
        "## Overview\n",
        "This notebook extracts high-quality tables from PDF files and formats them for Excel. Perfect for processing OCR-ed documents with structured data.\n",
        "\n",
        "## How to Use\n",
        "1. Click **\"Install Dependencies\"** button once\n",
        "2. Click **\"Extract Tables from PDF\"** button for each file\n",
        "3. Copy the displayed table and paste into Excel\n",
        "4. Use **\"Clear Workspace\"** button between files\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "T1S0YCAyk-2K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wfhsG6vbk4x1",
        "outputId": "699bbfba-0aa9-4ae1-90a9-5904f5d64340",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Installing enhanced packages with OCR support...\n",
            "‚è≥ This may take 2-3 minutes...\n",
            "üì¶ Installing system packages...\n",
            "üêç Installing Python packages...\n",
            "\n",
            "üîç Verifying installations...\n",
            "‚úÖ All packages installed successfully!\n",
            "‚úÖ Tesseract OCR ready\n",
            "\n",
            "üëá Ready! Scroll down to extract tables from PDFs\n"
          ]
        }
      ],
      "source": [
        "#@title üîß Install Dependencies - Enhanced OCR Version (Run Once) { display-mode: \"form\" }\n",
        "#@markdown Click Run to install all packages including OCR support. Takes 2-3 minutes.\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_packages():\n",
        "    print(\"üîß Installing enhanced packages with OCR support...\")\n",
        "    print(\"‚è≥ This may take 2-3 minutes...\")\n",
        "\n",
        "    # Install system dependencies\n",
        "    print(\"üì¶ Installing system packages...\")\n",
        "    subprocess.run(['apt-get', 'update', '-qq'], capture_output=True)\n",
        "    subprocess.run(['apt-get', 'install', '-y',\n",
        "                   'ghostscript',\n",
        "                   'python3-tk',\n",
        "                   'tesseract-ocr',           # OCR engine\n",
        "                   'tesseract-ocr-eng',       # English language data\n",
        "                   'poppler-utils',           # PDF utilities\n",
        "                   'ffmpeg',                  # Media processing\n",
        "                   'libsm6',                  # OpenCV dependencies\n",
        "                   'libxext6',\n",
        "                   '-qq'], capture_output=True)\n",
        "\n",
        "    # Install Python packages\n",
        "    print(\"üêç Installing Python packages...\")\n",
        "    packages = [\n",
        "        'camelot-py[cv]',\n",
        "        'tabula-py',\n",
        "        'pandas',\n",
        "        'openpyxl',\n",
        "        'pdfplumber',              # Better OCR PDF handling\n",
        "        'pymupdf',                 # PDF to image conversion\n",
        "        'pytesseract',             # OCR Python wrapper\n",
        "        'opencv-python-headless',  # Image preprocessing\n",
        "        'Pillow',                  # Image handling\n",
        "        'numpy'                    # Array operations\n",
        "    ]\n",
        "\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'install'] + packages + ['-q'],\n",
        "                  capture_output=True)\n",
        "\n",
        "    # Set display options\n",
        "    import pandas as pd\n",
        "    pd.set_option('display.max_rows', 20)\n",
        "    pd.set_option('display.max_columns', 10)\n",
        "    pd.set_option('display.width', None)\n",
        "    pd.set_option('display.max_colwidth', 50)\n",
        "\n",
        "    # Verify installations\n",
        "    print(\"\\nüîç Verifying installations...\")\n",
        "    try:\n",
        "        import camelot\n",
        "        import tabula\n",
        "        import pdfplumber\n",
        "        import fitz\n",
        "        import cv2\n",
        "        import pytesseract\n",
        "        print(\"‚úÖ All packages installed successfully!\")\n",
        "\n",
        "        # Check Tesseract\n",
        "        tesseract_version = subprocess.run(['tesseract', '--version'],\n",
        "                                         capture_output=True, text=True)\n",
        "        if tesseract_version.returncode == 0:\n",
        "            print(\"‚úÖ Tesseract OCR ready\")\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"‚ö†Ô∏è  Some packages failed: {e}\")\n",
        "        print(\"Try running the cell again\")\n",
        "\n",
        "    print(\"\\nüëá Ready! Scroll down to extract tables from PDFs\")\n",
        "\n",
        "install_packages()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üìä Enhanced PDF Table Extractor (OCR-Ready) { display-mode: \"form\" }\n",
        "#@markdown Advanced extraction with OCR support and multiple fallback methods\n",
        "\n",
        "import camelot\n",
        "import tabula\n",
        "import pandas as pd\n",
        "import pdfplumber\n",
        "import fitz  # PyMuPDF\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from google.colab import files\n",
        "import tempfile\n",
        "import os\n",
        "import io\n",
        "\n",
        "# Install required packages (run once)\n",
        "# !pip install camelot-py[cv] tabula-py pdfplumber pymupdf pytesseract opencv-python-headless\n",
        "\n",
        "def preprocess_image_for_ocr(image):\n",
        "    \"\"\"Enhance image quality for better OCR\"\"\"\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Denoise\n",
        "    denoised = cv2.fastNlMeansDenoising(gray)\n",
        "\n",
        "    # Adaptive thresholding for better text detection\n",
        "    thresh = cv2.adaptiveThreshold(denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                   cv2.THRESH_BINARY, 11, 2)\n",
        "\n",
        "    # Deskew if needed\n",
        "    coords = np.column_stack(np.where(thresh > 0))\n",
        "    angle = cv2.minAreaRect(coords)[-1]\n",
        "    if angle < -45:\n",
        "        angle = 90 + angle\n",
        "    if abs(angle) > 0.5:\n",
        "        (h, w) = thresh.shape[:2]\n",
        "        center = (w // 2, h // 2)\n",
        "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "        thresh = cv2.warpAffine(thresh, M, (w, h),\n",
        "                               flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
        "\n",
        "    return thresh\n",
        "\n",
        "def extract_with_pdfplumber(file_path):\n",
        "    \"\"\"Extract using pdfplumber - good for OCR-ed PDFs\"\"\"\n",
        "    tables_data = []\n",
        "    try:\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            for page_num, page in enumerate(pdf.pages, 1):\n",
        "                tables = page.extract_tables(table_settings={\n",
        "                    \"vertical_strategy\": \"lines_strict\",\n",
        "                    \"horizontal_strategy\": \"lines_strict\",\n",
        "                    \"explicit_vertical_lines\": [],\n",
        "                    \"explicit_horizontal_lines\": [],\n",
        "                    \"snap_tolerance\": 3,\n",
        "                    \"join_tolerance\": 3,\n",
        "                    \"edge_min_length\": 5,\n",
        "                    \"min_words_vertical\": 1,\n",
        "                    \"min_words_horizontal\": 1,\n",
        "                    \"text_tolerance\": 3,\n",
        "                    \"text_x_tolerance\": None,\n",
        "                    \"text_y_tolerance\": None,\n",
        "                })\n",
        "\n",
        "                for i, table in enumerate(tables):\n",
        "                    if table:\n",
        "                        df = pd.DataFrame(table)\n",
        "                        # Clean empty rows/columns\n",
        "                        df = df.dropna(how='all').dropna(axis=1, how='all')\n",
        "                        if not df.empty:\n",
        "                            tables_data.append({\n",
        "                                'dataframe': df,\n",
        "                                'page': page_num,\n",
        "                                'method': 'pdfplumber'\n",
        "                            })\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå pdfplumber error: {str(e)}\")\n",
        "    return tables_data\n",
        "\n",
        "def extract_with_ocr_pymupdf(file_path):\n",
        "    \"\"\"Direct OCR extraction using PyMuPDF and Tesseract\"\"\"\n",
        "    tables_data = []\n",
        "    try:\n",
        "        pdf_document = fitz.open(file_path)\n",
        "\n",
        "        for page_num, page in enumerate(pdf_document, 1):\n",
        "            # Convert page to image\n",
        "            mat = fitz.Matrix(2, 2)  # 2x zoom for better OCR\n",
        "            pix = page.get_pixmap(matrix=mat)\n",
        "            img_data = pix.pil_tobytes(format=\"PNG\")\n",
        "            image = Image.open(io.BytesIO(img_data))\n",
        "\n",
        "            # Preprocess image\n",
        "            processed = preprocess_image_for_ocr(image)\n",
        "\n",
        "            # OCR with table detection hints\n",
        "            custom_config = r'--oem 3 --psm 6 -c preserve_interword_spaces=1'\n",
        "            text = pytesseract.image_to_data(processed, output_type=pytesseract.Output.DICT,\n",
        "                                           config=custom_config)\n",
        "\n",
        "            # Group text by lines and detect table structure\n",
        "            df = pd.DataFrame(text)\n",
        "            df = df[df['conf'] > 30]  # Filter low confidence\n",
        "\n",
        "            if not df.empty:\n",
        "                # Simple table reconstruction based on position\n",
        "                df['line_group'] = df['line_num']\n",
        "                table_data = []\n",
        "\n",
        "                for line_num in df['line_group'].unique():\n",
        "                    line_data = df[df['line_group'] == line_num]\n",
        "                    if len(line_data) > 1:  # Multiple words in line = potential table row\n",
        "                        row = line_data.sort_values('left')['text'].tolist()\n",
        "                        table_data.append(row)\n",
        "\n",
        "                if len(table_data) > 1:  # At least 2 rows\n",
        "                    result_df = pd.DataFrame(table_data)\n",
        "                    tables_data.append({\n",
        "                        'dataframe': result_df,\n",
        "                        'page': page_num,\n",
        "                        'method': 'OCR-PyMuPDF'\n",
        "                    })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå OCR extraction error: {str(e)}\")\n",
        "    return tables_data\n",
        "\n",
        "def extract_tables_enhanced(file_path):\n",
        "    \"\"\"Multi-method extraction with fallbacks\"\"\"\n",
        "    all_tables = []\n",
        "\n",
        "    # Method 1: Camelot with both lattice and stream\n",
        "    print(\"üéØ Trying Camelot (lattice)...\")\n",
        "    try:\n",
        "        tables = camelot.read_pdf(file_path, flavor='lattice', pages='all',\n",
        "                                 line_scale=40, copy_text=['v', 'h'])\n",
        "        for table in tables:\n",
        "            if not table.df.empty:\n",
        "                all_tables.append({\n",
        "                    'dataframe': table.df,\n",
        "                    'page': table.page,\n",
        "                    'accuracy': table.accuracy,\n",
        "                    'method': 'camelot-lattice'\n",
        "                })\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    if len(all_tables) == 0:\n",
        "        print(\"üéØ Trying Camelot (stream)...\")\n",
        "        try:\n",
        "            tables = camelot.read_pdf(file_path, flavor='stream', pages='all',\n",
        "                                     edge_tol=50, row_tol=10, column_tol=10)\n",
        "            for table in tables:\n",
        "                if not table.df.empty:\n",
        "                    all_tables.append({\n",
        "                        'dataframe': table.df,\n",
        "                        'page': table.page,\n",
        "                        'accuracy': table.accuracy,\n",
        "                        'method': 'camelot-stream'\n",
        "                    })\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Method 2: pdfplumber (good for OCR)\n",
        "    if len(all_tables) < 3:  # Try if few tables found\n",
        "        print(\"üéØ Trying pdfplumber...\")\n",
        "        plumber_tables = extract_with_pdfplumber(file_path)\n",
        "        all_tables.extend(plumber_tables)\n",
        "\n",
        "    # Method 3: Tabula with different strategies\n",
        "    if len(all_tables) < 3:\n",
        "        print(\"üéØ Trying Tabula...\")\n",
        "        try:\n",
        "            # Try different extraction methods\n",
        "            for lattice in [True, False]:\n",
        "                tables = tabula.read_pdf(file_path, pages='all',\n",
        "                                       multiple_tables=True, lattice=lattice)\n",
        "                for i, table in enumerate(tables):\n",
        "                    if not table.empty:\n",
        "                        all_tables.append({\n",
        "                            'dataframe': table,\n",
        "                            'page': 'auto',\n",
        "                            'accuracy': 0,\n",
        "                            'method': f'tabula-{\"lattice\" if lattice else \"stream\"}'\n",
        "                        })\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Method 4: Direct OCR if still no good results\n",
        "    if len(all_tables) < 2:\n",
        "        print(\"üéØ Applying direct OCR extraction...\")\n",
        "        ocr_tables = extract_with_ocr_pymupdf(file_path)\n",
        "        all_tables.extend(ocr_tables)\n",
        "\n",
        "    return all_tables\n",
        "\n",
        "def merge_similar_tables(tables, similarity_threshold=0.7):\n",
        "    \"\"\"Merge duplicate tables from different methods\"\"\"\n",
        "    unique_tables = []\n",
        "\n",
        "    for table in tables:\n",
        "        is_duplicate = False\n",
        "        df = table['dataframe']\n",
        "\n",
        "        for unique in unique_tables:\n",
        "            unique_df = unique['dataframe']\n",
        "            # Check if tables are similar (same shape and content overlap)\n",
        "            if df.shape == unique_df.shape:\n",
        "                try:\n",
        "                    overlap = (df == unique_df).sum().sum() / (df.shape[0] * df.shape[1])\n",
        "                    if overlap > similarity_threshold:\n",
        "                        is_duplicate = True\n",
        "                        # Keep the one with higher accuracy\n",
        "                        if table.get('accuracy', 0) > unique.get('accuracy', 0):\n",
        "                            unique_tables.remove(unique)\n",
        "                            unique_tables.append(table)\n",
        "                        break\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        if not is_duplicate:\n",
        "            unique_tables.append(table)\n",
        "\n",
        "    return unique_tables\n",
        "\n",
        "def format_for_excel_enhanced(tables):\n",
        "    \"\"\"Enhanced formatting with metadata\"\"\"\n",
        "    if not tables:\n",
        "        return None\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for i, table_info in enumerate(tables):\n",
        "        df = table_info['dataframe']\n",
        "        if not df.empty:\n",
        "            # Clean data\n",
        "            df = df.replace('', np.nan)\n",
        "            df = df.dropna(how='all').dropna(axis=1, how='all')\n",
        "\n",
        "            if not df.empty:\n",
        "                df_with_meta = df.copy()\n",
        "                df_with_meta.insert(0, 'Table_#', i + 1)\n",
        "                df_with_meta.insert(1, 'Page', table_info.get('page', 'N/A'))\n",
        "                df_with_meta.insert(2, 'Method', table_info.get('method', 'N/A'))\n",
        "                df_with_meta.insert(3, 'Quality', f\"{table_info.get('accuracy', 0):.1f}%\")\n",
        "                all_data.append(df_with_meta)\n",
        "\n",
        "    return pd.concat(all_data, ignore_index=True) if all_data else None\n",
        "\n",
        "# Main processing\n",
        "print(\"üìÅ Select your PDF file (OCR-ready):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    for filename, file_content in uploaded.items():\n",
        "        print(f\"\\nüîÑ Processing: {filename}\")\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
        "            tmp_file.write(file_content)\n",
        "            tmp_file_path = tmp_file.name\n",
        "\n",
        "        try:\n",
        "            # Extract tables with all methods\n",
        "            all_tables = extract_tables_enhanced(tmp_file_path)\n",
        "\n",
        "            # Remove duplicates\n",
        "            unique_tables = merge_similar_tables(all_tables)\n",
        "\n",
        "            if unique_tables:\n",
        "                print(f\"\\n‚úÖ Found {len(unique_tables)} unique tables\")\n",
        "                for i, table in enumerate(unique_tables):\n",
        "                    print(f\"   Table {i+1}: Page {table.get('page', 'N/A')}, \"\n",
        "                          f\"Method: {table.get('method', 'N/A')}, \"\n",
        "                          f\"Quality: {table.get('accuracy', 0):.1f}%\")\n",
        "\n",
        "                # Create Excel file\n",
        "                excel_df = format_for_excel_enhanced(unique_tables)\n",
        "\n",
        "                if excel_df is not None and not excel_df.empty:\n",
        "                    base_name = filename.replace('.pdf', '')\n",
        "                    excel_filename = f\"{base_name}_extracted_tables.xlsx\"\n",
        "\n",
        "                    # Save with multiple sheets if needed\n",
        "                    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
        "                        # Main sheet with all tables\n",
        "                        excel_df.to_excel(writer, sheet_name='All_Tables', index=False)\n",
        "\n",
        "                        # Individual sheets for each table\n",
        "                        for i, table in enumerate(unique_tables):\n",
        "                            df = table['dataframe']\n",
        "                            sheet_name = f\"Table_{i+1}_P{table.get('page', 'X')}\"[:31]\n",
        "                            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "                    files.download(excel_filename)\n",
        "                    print(f\"\\nüíæ Downloaded: {excel_filename}\")\n",
        "                    print(\"‚úÖ Processing complete!\")\n",
        "                else:\n",
        "                    print(\"‚ùå No valid tables extracted\")\n",
        "            else:\n",
        "                print(\"‚ùå No tables found in PDF\")\n",
        "                print(\"üí° The PDF might need manual review or different OCR settings\")\n",
        "\n",
        "        finally:\n",
        "            os.unlink(tmp_file_path)\n",
        "else:\n",
        "    print(\"‚ùå No file uploaded\")"
      ],
      "metadata": {
        "id": "BzIoHgIGlDEM",
        "outputId": "a2f7ad01-7a8b-44e1-ba18-b2486e9a0cee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Select your PDF file (Advanced OCR-ready):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1b43f079-953d-401e-9bb1-ffd8b741046d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1b43f079-953d-401e-9bb1-ffd8b741046d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Sample D_EN 1.pdf to Sample D_EN 1 (1).pdf\n",
            "\n",
            "üîÑ Processing: Sample D_EN 1 (1).pdf\n",
            "üîç Using multi-pass extraction with merged cell detection...\n",
            "üéØ Pass 1: Standard extraction...\n",
            "üéØ Pass 2: Enhanced extraction with merged cell handling...\n",
            "üéØ Pass 3: Advanced OCR extraction...\n",
            "\n",
            "‚úÖ Found 2 unique tables\n",
            "   Table 1: Table_1_P1_camelot - 22 rows √ó 8 cols\n",
            "   Table 2: Table_2_P1_OCR - 33 rows √ó 17 cols\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-7-2121084215.py:409: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df_normalized = df.replace('', np.nan).dropna(how='all').dropna(axis=1, how='all')\n",
            "/tmp/ipython-input-7-2121084215.py:413: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  unique_normalized = unique_df.replace('', np.nan).dropna(how='all').dropna(axis=1, how='all')\n",
            "/tmp/ipython-input-7-2121084215.py:474: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.replace('', np.nan).dropna(how='all').dropna(axis=1, how='all')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_efc66945-e2ec-4df2-80f7-6034b1392f51\", \"Sample D_EN 1 (1)_extracted_tables.xlsx\", 8831)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üíæ Downloaded: Sample D_EN 1 (1)_extracted_tables.xlsx\n",
            "‚úÖ Processing complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üóëÔ∏è Clear Input & Output Directory\n",
        "\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Define the directories to be cleared\n",
        "directories_to_clear = [\"/content/sample_data\"]\n",
        "\n",
        "# Also clear uploaded files in /content (but preserve system folders)\n",
        "content_dir = Path(\"/content\")\n",
        "system_folders = {\".config\", \"sample_data\", \"__pycache__\"}\n",
        "\n",
        "# Warning message\n",
        "print(\"‚ö†Ô∏è WARNING: This will delete all contents of the following directories:\")\n",
        "for directory in directories_to_clear:\n",
        "    print(f\"- {directory}\")\n",
        "print(\"- Uploaded files in /content (excluding system folders)\")\n",
        "\n",
        "#confirmation = input(\"Type 'YES' to confirm: \")\n",
        "\n",
        "if True:\n",
        "    # Clear specified directories\n",
        "    for directory in directories_to_clear:\n",
        "        dir_path = Path(directory)\n",
        "        if dir_path.exists() and dir_path.is_dir():\n",
        "            shutil.rmtree(dir_path)\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)\n",
        "            print(f\"‚úÖ '{directory}' has been cleared.\")\n",
        "        else:\n",
        "            print(f\"The '{directory}' directory does not exist.\")\n",
        "\n",
        "    # Clear uploaded files from /content\n",
        "    if content_dir.exists():\n",
        "        for item in content_dir.iterdir():\n",
        "            if item.name not in system_folders and item.is_file():\n",
        "                item.unlink()\n",
        "                print(f\"‚úÖ Removed uploaded file: {item.name}\")\n",
        "            elif item.is_dir() and item.name not in system_folders and item.name not in [\"output\", \"sample_pdfs\"]:\n",
        "                shutil.rmtree(item)\n",
        "                print(f\"‚úÖ Removed uploaded folder: {item.name}\")\n",
        "\n",
        "else:\n",
        "    print(\"Operation cancelled. No files were deleted.\")"
      ],
      "metadata": {
        "id": "JyAINkM3lHaS",
        "cellView": "form",
        "outputId": "fae6fa41-7d4e-4e3a-8976-206be4998ba5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è WARNING: This will delete all contents of the following directories:\n",
            "- /content/sample_data\n",
            "- Uploaded files in /content (excluding system folders)\n",
            "‚úÖ '/content/sample_data' has been cleared.\n",
            "‚úÖ Removed uploaded file: Sample D_EN 1.pdf\n",
            "‚úÖ Removed uploaded file: Sample D_EN 1_tables.xlsx\n"
          ]
        }
      ]
    }
  ]
}